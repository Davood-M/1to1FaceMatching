{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import shutil\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch_mtcnn import detect_faces\n",
    "from PIL import Image\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device = %s' %(torch.cuda.get_device_name(device) if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_1(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path, max_samples, identity_imgs=10):\n",
    "        paths = {}\n",
    "\n",
    "        for gender in os.listdir(folder_path):\n",
    "            gender_path = os.path.join(folder_path, gender)\n",
    "            if os.path.isdir(gender_path):\n",
    "                paths[gender] = {} # contains identities\n",
    "\n",
    "                for iden in os.listdir(gender_path):\n",
    "                    iden_path = os.path.join( gender_path, iden )\n",
    "                    if not os.path.isdir(iden_path):\n",
    "                        continue\n",
    "\n",
    "                    # add some pictures form that specific identity\n",
    "                    paths[gender][iden] = []\n",
    "                    for i in os.listdir(iden_path):\n",
    "                        if os.path.isdir( os.path.join(iden_path, i) ):\n",
    "                            continue\n",
    "                        if len(paths[gender][iden]) >= identity_imgs:\n",
    "                            break\n",
    "                        paths[gender][iden].append(os.path.join(iden_path, i))\n",
    "\n",
    "        # create random data\n",
    "        self.pairs = []\n",
    "        for i in tqdm(range(max_samples)):\n",
    "            # select a gender -> can be changed to make the dataset biased\n",
    "            if random.random() <= 0.5:\n",
    "                # label 1: same identities\n",
    "                gender = random.choice(list(paths.keys()))\n",
    "                iden = random.choice(list(paths[gender].keys()))\n",
    "                img_1 = random.choice(paths[gender][iden])\n",
    "                img_2 = random.choice(paths[gender][iden])\n",
    "                self.pairs.append((\n",
    "                    img_1,\n",
    "                    img_2,\n",
    "                    0,\n",
    "                    gender, gender,\n",
    "                    img_1, img_2, 'VGG_1'\n",
    "                ))\n",
    "            else:\n",
    "                # label 0: different identities and maybe different genders as well!\n",
    "                gender_1 = random.choice(list(paths.keys()))\n",
    "                iden_1 = random.choice(list(paths[gender_1].keys()))\n",
    "                gender_2 = random.choice(list(paths.keys()))\n",
    "                iden_2 = random.choice(list(paths[gender_2].keys()))\n",
    "                img_1 = random.choice(paths[gender_1][iden_1])\n",
    "                img_2 = random.choice(paths[gender_2][iden_2])\n",
    "                self.pairs.append((\n",
    "                    img_1,\n",
    "                    img_2,\n",
    "                    1,\n",
    "                    gender_1,\n",
    "                    gender_2,\n",
    "                    img_1,\n",
    "                    img_2, 'VGG_1'\n",
    "                ))\n",
    "        # print data dist.\n",
    "        counts = {1:0, 0:0}\n",
    "        for p in self.pairs:\n",
    "            counts[p[2]] += 1\n",
    "        print('\\n\\nCounts 0 = %d | 1 = %d' %(counts[0], counts[1]))\n",
    "\n",
    "        # preload images if needed!\n",
    "        self.img_pairs = []\n",
    "        self.loaded = False\n",
    "\n",
    "    def load_images(self):\n",
    "        norm_img = np.zeros((100, 100))\n",
    "\n",
    "        # load the images from self.pairs\n",
    "        for pair in tqdm(self.pairs):\n",
    "            img_1 = cv2.imread(pair[0])\n",
    "            img_2 = cv2.imread(pair[1])\n",
    "\n",
    "            # self.img_pairs.append((\n",
    "                # cv2.normalize(cv2.resize(cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY), (100, 100)), norm_img, 0, 255, cv2.NORM_MINMAX),\n",
    "                # cv2.normalize(cv2.resize(cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY), (100, 100)), norm_img, 0, 255, cv2.NORM_MINMAX),\n",
    "                # pair[2], pair[3], pair[4], pair[5], pair[6]\n",
    "            # ))\n",
    "            self.img_pairs.append((\n",
    "                cv2.normalize(cv2.resize(img_1, (100, 100)), norm_img, 0, 255, cv2.NORM_MINMAX),\n",
    "                cv2.normalize(cv2.resize(img_2, (100, 100)), norm_img, 0, 255, cv2.NORM_MINMAX),\n",
    "                pair[2], pair[3], pair[4], pair[5], pair[6]\n",
    "            ))\n",
    "        self.loaded = True\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        if not self.loaded:\n",
    "            return self.pairs[ind]\n",
    "        return self.img_pairs[ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "\n",
    "##########   FaceDatabases   ##########\n",
    "class FaceDB(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_dict, pre_load=False):\n",
    "        '''\n",
    "        path_dict = {\n",
    "            'iranian_women': {path: str, samples: int},\n",
    "            'multiview_01' : {path: str, samples: int}\n",
    "            'vgg_1         : {path: str, samples: int},\n",
    "        }\n",
    "        '''\n",
    "\n",
    "        self.pairs = []\n",
    "\n",
    "        # get the VGG data by default\n",
    "        dataset = VGG_1(path_dict['vgg_1']['path'], path_dict['vgg_1']['samples'])\n",
    "        self.pairs.extend(dataset.pairs)\n",
    "        del dataset\n",
    "\n",
    "        # read from other databases\n",
    "        self.pairs.extend(self.iranian_woman_db(path_dict['morphed']['path'], path_dict['morphed']['samples'], 'morphed'))\n",
    "        self.pairs.extend(self.multi_pie_multiview(path_dict['multiview_01']['path'], path_dict['multiview_01']['samples'], 'multiview_01'))\n",
    "        self.pairs.extend(self.iranian_woman_db(path_dict['iranian_women']['path'], path_dict['iranian_women']['samples'], 'iranian_women'))\n",
    "        self.pairs.extend(self.multi_pie_multiview(path_dict['multiview_02']['path'], path_dict['multiview_02']['samples'], 'multiview_02'))\n",
    "\n",
    "        # print data dist.\n",
    "        counts = {1:0, 0:0}\n",
    "        for p in self.pairs:\n",
    "            counts[p[2]] += 1\n",
    "        print('\\n\\nCounts 0 = %d | 1 = %d' %(counts[0], counts[1]))\n",
    "\n",
    "        # preload images if needed!\n",
    "        self.img_pairs = []\n",
    "        self.loaded = False\n",
    "\n",
    "    def multi_pie_multiview(self, path, no_samples, name, camera_list=['13_0', '14_0', '05_1', '05_0', '04_1'], session='01'):\n",
    "        # get the identities\n",
    "        identities = {}\n",
    "        pairs = []\n",
    "\n",
    "        for iden in os.listdir(path):\n",
    "            if not os.path.isfile(iden):\n",
    "                # consider only session 01 for now\n",
    "                images = []\n",
    "                for camera in camera_list:\n",
    "                    for i in os.listdir(os.path.join(path, iden, session, camera)):\n",
    "                        images.append(os.path.join(path, iden, session, camera, i))\n",
    "                if len(images) > 0:\n",
    "                    identities[iden] = images\n",
    "\n",
    "        # loop to create image pairs\n",
    "        for i in tqdm(range(no_samples)):\n",
    "            if random.random() <= 0.5:\n",
    "                # label 1: same identities\n",
    "                iden = random.choice(list(identities.keys()))\n",
    "                img_1 = random.choice(identities[iden])\n",
    "                img_2 = random.choice(identities[iden])\n",
    "                pairs.append((\n",
    "                    img_1,\n",
    "                    img_2,\n",
    "                    0,\n",
    "                    'unknown', 'unknown',\n",
    "                    img_1, img_2, name + session\n",
    "                ))\n",
    "            else:\n",
    "                # label 0: different identities and maybe different genders as well!\n",
    "                iden_1 = random.choice(list(identities.keys()))\n",
    "                iden_2 = random.choice(list(identities.keys()))\n",
    "                while iden_1 == iden_2:\n",
    "                    iden_2 = random.choice(list(identities.keys()))\n",
    "\n",
    "                img_1 = random.choice(identities[iden_1])\n",
    "                img_2 = random.choice(identities[iden_2])\n",
    "                pairs.append((\n",
    "                    img_1,\n",
    "                    img_2,\n",
    "                    1,\n",
    "                    'unknown', 'unknown',\n",
    "                    img_1,\n",
    "                    img_2, name + session\n",
    "                ))\n",
    "        return pairs\n",
    "\n",
    "    def iranian_woman_db(self, path, no_samples, name):\n",
    "        # get the identities\n",
    "        identities = {}\n",
    "        pairs = []\n",
    "\n",
    "        for iden in os.listdir(path):\n",
    "            if not os.path.isfile(iden):\n",
    "                # identities.append(os.path.join(path, iden))\n",
    "                images = []\n",
    "                for i in os.listdir(os.path.join(path, iden)):\n",
    "                    if not ('_2' in i or '_3' in i or '_4' in i or '_5' in i):\n",
    "                        images.append(os.path.join(path, iden, i))\n",
    "                identities[iden] = images\n",
    "\n",
    "        # loop to create image pairs\n",
    "        for i in tqdm(range(no_samples)):\n",
    "            if random.random() <= 0.5:\n",
    "                # label 1: same identities\n",
    "                iden = random.choice(list(identities.keys()))\n",
    "                img_1 = random.choice(identities[iden])\n",
    "                img_2 = random.choice(identities[iden])\n",
    "                pairs.append((\n",
    "                    img_1,\n",
    "                    img_2,\n",
    "                    0,\n",
    "                    'female', 'female',\n",
    "                    img_1, img_2, name\n",
    "                ))\n",
    "            else:\n",
    "                # label 0: different identities and maybe different genders as well!\n",
    "                iden_1 = random.choice(list(identities.keys()))\n",
    "                iden_2 = random.choice(list(identities.keys()))\n",
    "                while iden_1 == iden_2:\n",
    "                    iden_2 = random.choice(list(identities.keys()))\n",
    "\n",
    "                img_1 = random.choice(identities[iden_1])\n",
    "                img_2 = random.choice(identities[iden_2])\n",
    "                pairs.append((\n",
    "                    img_1, img_2,\n",
    "                    1,\n",
    "                    'female', 'female',\n",
    "                    img_1,\n",
    "                    img_2, name\n",
    "                ))\n",
    "        return pairs\n",
    "\n",
    "    def crop_image(self, path):\n",
    "        image = cv2.imread(path)\n",
    "        res = detect_faces(image)\n",
    "        if res is None:\n",
    "            return None\n",
    "        bounding_boxes, landmarks = res[0], res[1]\n",
    "\n",
    "        if len(bounding_boxes) == 0 or len(bounding_boxes) > 1:\n",
    "            return None\n",
    "\n",
    "        # else, crop the image\n",
    "        bounding_boxes = list(map(int, bounding_boxes[0]))\n",
    "        return image[ bounding_boxes[1] : bounding_boxes[3], bounding_boxes[0] : bounding_boxes[2]]\n",
    "\n",
    "    def load_images(self):\n",
    "        norm_img = np.zeros((100, 100))\n",
    "        self.idx = {}\n",
    "        idx_pair = -1\n",
    "        idx_loaded = 0\n",
    "\n",
    "        # load the images from self.pairs\n",
    "        for pair in tqdm(self.pairs):\n",
    "            idx_pair += 1\n",
    "            # crop image if needed\n",
    "            if 'VGG_1' == pair[7]:\n",
    "                img_1 = cv2.imread(pair[0])\n",
    "                img_2 = cv2.imread(pair[1])\n",
    "            else:\n",
    "                img_1 = self.crop_image(pair[0])\n",
    "                img_2 = self.crop_image(pair[1])\n",
    "\n",
    "            if img_1 is None or img_2 is None:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # self.img_pairs.append((\n",
    "                #    cv2.normalize(cv2.resize(cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY), (100, 100)), norm_img, 0, 255, cv2.NORM_MINMAX),\n",
    "                #    cv2.normalize(cv2.resize(cv2.cvtColor(img_2, cv2.COLOR_BGR2GRAY), (100, 100)), norm_img, 0, 255, cv2.NORM_MINMAX),\n",
    "                #    pair[2], pair[3], pair[4], pair[5], pair[6], pair[7]\n",
    "                # ))\n",
    "                self.img_pairs.append((\n",
    "                    cv2.normalize(cv2.resize(img_1, (100, 100)), norm_img, 0, 255, cv2.NORM_MINMAX),\n",
    "                    cv2.normalize(cv2.resize(img_2, (100, 100)), norm_img, 0, 255, cv2.NORM_MINMAX),\n",
    "                    pair[2], pair[3], pair[4], pair[5], pair[6], pair[7]\n",
    "                ))\n",
    "                self.idx[idx_loaded] = idx_pair\n",
    "                idx_loaded += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        self.loaded = True\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        if not self.loaded:\n",
    "            return self.pairs[ind]\n",
    "        return self.img_pairs[ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./data_cache_rgb.pkl'):\n",
    "    path_dict = {\n",
    "        'iranian_women': {'path': 'D:\\\\Dropbox Main\\\\Dropbox\\\\datasets\\\\12_Iranianwomendb', 'samples': 1},\n",
    "        'multiview_01' : {'path': 'data\\\\session01\\\\multiview', 'samples': 1},\n",
    "        'vgg_1'        : {'path': 'dataset', 'samples': 200000},\n",
    "        'multiview_02' : {'path': 'data\\\\session02\\\\multiview', 'samples': 1},\n",
    "        'morphed': {'path': 'D:\\\\Dropbox Main\\\\Dropbox\\\\datasets\\\\1_Morphdb', 'samples': 1}\n",
    "    }\n",
    "\n",
    "    dataset = FaceDB(path_dict)\n",
    "    dataset.load_images()\n",
    "    pickle.dump(dataset, open('data_cache_rgb.pkl', 'wb'))\n",
    "else:\n",
    "    dataset = pickle.load(open('data_cache_rgb.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some of the pictures\n",
    "for i in range(5):\n",
    "    r_ind = random.randint(0, len(dataset) - 1)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(dataset[r_ind][0])\n",
    "    plt.subplot(122)\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(75, 8, 'label %d' %dataset[r_ind][2], style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(dataset[r_ind][1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into different sets -> train and test for now\n",
    "test_portion = 0.2\n",
    "\n",
    "test_size = int(test_portion * len(dataset))\n",
    "train_size = len(dataset) - test_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "print('Loaders Size = %d | %d' %(len(train_loader), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.img_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese_Net, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 10),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Conv2d(64, 128, 7),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Conv2d(128, 128, 4),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            # FC\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4096, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        o1, o2 = self.net(x1), self.net(x2)\n",
    "        return self.classifier( torch.abs(o1 - o2) )\n",
    "\n",
    "# define the loss function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "      def __init__(self, margin=2.0):\n",
    "            super(ContrastiveLoss, self).__init__()\n",
    "            self.margin = margin\n",
    "\n",
    "      def forward(self, output1, output2, label):\n",
    "            # Find the pairwise distance\n",
    "            euclidean_distance = torch.functional.F.pairwise_distance(output1, output2)\n",
    "            # perform contrastive loss calculation with the distance\n",
    "            loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) + (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "            return loss_contrastive\n",
    "\n",
    "def test(model, data_loader):\n",
    "    model.eval()\n",
    "    preds_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        preds = model(batch[0].float().permute(0, 3, 1, 2).to(device), batch[1].float().permute(0, 3, 1, 2).to(device))\n",
    "        dists = torch.ones(preds.shape)\n",
    "        dists[torch.where(torch.abs(preds) < 0.5)] = 0\n",
    "        dists[torch.where(torch.abs(preds) > 0.5)] = 1\n",
    "        preds_labels.extend(dists.squeeze(1).tolist())\n",
    "        true_labels.extend( batch[2].tolist() )\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, preds_labels).ravel()\n",
    "    return (tp + tn) / (tn + fp + fn + tp)\n",
    "\n",
    "# create the model\n",
    "model = Siamese_Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 25\n",
    "loss_history = []\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=6e-4)\n",
    "loss_func = nn.CrossEntropyLoss() # ContrastiveLoss()\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    model.train()\n",
    "\n",
    "    for batch in tqdm(train_loader, desc='Batch %d' %i, total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x0 = batch[0].float().permute(0, 3, 1, 2).to(device)\n",
    "        x1 = batch[1].float().permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "        preds = model(x0, x1).squeeze(-1)\n",
    "        # calc loss\n",
    "        loss = loss_func(preds, batch[2].to(device))\n",
    "\n",
    "        # update model's params\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "    # test the model after several steps\n",
    "    print('Iteration %d - Loss = %0.8f' %(i, loss.item()))\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if i != 0 and i % 2 == 0:\n",
    "        # save the model\n",
    "        # torch.save(model.state_dict(), './model_%d.pt' %(i))\n",
    "        # save the model\n",
    "        torch.save({\n",
    "            'epoch': i,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, 'model_rgb_v2_%d.pt' %(i))\n",
    "        #print('\\tAccuracies = %0.3f | %0.3f' %(test(model, train_loader), test(model, test_loader)))\n",
    "#print('\\tAccuracies = %0.3f | %0.3f' %(test(model, train_loader), test(model, test_loader)))\n",
    "\n",
    "# save the model\n",
    "torch.save({\n",
    "    'epoch': i,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss\n",
    "    }, 'model_rgb_v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the loss hostory\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def imshow(img, text=None, should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.subplot(121)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(npimg[0])\n",
    "    plt.subplot(122)\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(npimg[1])\n",
    "    plt.show()\n",
    "\n",
    "# load the model\n",
    "model = Siamese_Net().to(device)\n",
    "checkpoint = torch.load('model_rgb.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1)\n",
    "dataiter = iter(test_dataloader)\n",
    "x0,_,_, _, _, _, _, _ = next(dataiter)\n",
    "next(dataiter)\n",
    "\n",
    "for i in range(20):\n",
    "    x0, x1, label, _, _, _, _, _ = next(dataiter)\n",
    "    concatenated = torch.cat((x0,x1), 0)\n",
    "    \n",
    "    preds = model(x0.float().permute(0, 3, 1, 2).to(device), x1.float().permute(0, 3, 1, 2).to(device))\n",
    "    imshow(concatenated,'Dissimilarity: {:.2f} | {:d}'.format(preds.to('cpu').detach().numpy()[0][0], label.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for iteration in range(5):\n",
    "    print('ITER', iteration)\n",
    "    # get current index\n",
    "    mis_dirs = list(map(int, [i for i in os.listdir('D:/Dropbox Main/Dropbox/datasets/Trials/mismatch/') if not os.path.isfile(os.path.join('D:/Dropbox Main/Dropbox/datasets/Trials/mismatch', i))]))\n",
    "    mismatch_idx = max(mis_dirs) + 1 if len(mis_dirs) > 0 else 0\n",
    "    m_dirs = list(map(int, [i for i in os.listdir('D:/Dropbox Main/Dropbox/datasets/Trials/match/') if not os.path.isfile(os.path.join('D:/Dropbox Main/Dropbox/datasets/Trials/match', i))]))\n",
    "    match_idx = max(m_dirs) + 1 if len(m_dirs) > 0 else 0\n",
    "    print(mismatch_idx, match_idx)\n",
    "\n",
    "    ## Get the required image pairs\n",
    "    model = Siamese_Net().to(device)\n",
    "    saved_weights = torch.load('./model.pt')\n",
    "    model.load_state_dict(saved_weights['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    # generate several images\n",
    "    path_dict = {\n",
    "        'iranian_women': {'path': 'D:\\\\Dropbox Main\\\\Dropbox\\\\datasets\\\\12_Iranianwomendb', 'samples': 0},\n",
    "        'multiview_01' : {'path': 'data\\\\session01\\\\multiview', 'samples': 1000},\n",
    "        'vgg_1'        : {'path': 'dataset', 'samples': 500},\n",
    "        'multiview_02' : {'path': 'data\\\\session02\\\\multiview', 'samples': 1000},\n",
    "        'morphed': {'path': 'D:\\\\Dropbox Main\\\\Dropbox\\\\datasets\\\\1_Morphdb', 'samples': 5000}\n",
    "    }\n",
    "\n",
    "    dataset = FaceDB(path_dict)\n",
    "    dataset.load_images()\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=64)\n",
    "\n",
    "    for batch in data_loader:\n",
    "        preds_labels = []\n",
    "        true_labels = []\n",
    "\n",
    "        preds = model(batch[0].unsqueeze(1).float().to(device), batch[1].unsqueeze(1).float().to(device))\n",
    "        dists = torch.ones(preds.shape)\n",
    "        dists[torch.where(torch.abs(preds) < 0.5)] = 0\n",
    "        dists[torch.where(torch.abs(preds) > 0.5)] = 1\n",
    "        preds_labels.extend(dists.squeeze(1).tolist())\n",
    "        true_labels.extend( batch[2].tolist() )\n",
    "\n",
    "        for idx in range(len(true_labels)):\n",
    "            if true_labels[idx] == 1 and preds_labels[idx] == 0 and abs(preds[idx] - 0.5) <= 0.3:\n",
    "                # copy files\n",
    "                os.mkdir('D:/Dropbox Main/Dropbox/datasets/Trials/mismatch/' + str(mismatch_idx))\n",
    "                shutil.copy(batch[5][idx], 'D:/Dropbox Main/Dropbox/datasets/Trials/mismatch/' + str(mismatch_idx) + '/' + batch[5][idx].split('\\\\')[-1])\n",
    "                shutil.copy(batch[6][idx], 'D:/Dropbox Main/Dropbox/datasets/Trials/mismatch/' + str(mismatch_idx) + '/' + batch[6][idx].split('\\\\')[-1])\n",
    "                fout = open('D:/Dropbox Main/Dropbox/datasets/Trials/mismatch/' + str(mismatch_idx) + '/info.txt', 'w')\n",
    "                fout.write('Gender = %s\\nDB Name = %s\\n' %(batch[3][idx], batch[-1][idx]))\n",
    "                fout.close()\n",
    "                mismatch_idx += 1\n",
    "\n",
    "            elif true_labels[idx] == 0 and preds_labels[idx] == 1 and abs(preds[idx] - 0.5) <= 0.1:\n",
    "                # copy files\n",
    "                os.mkdir('D:/Dropbox Main/Dropbox/datasets/Trials/match/' + str(match_idx))\n",
    "                shutil.copy(batch[5][idx], 'D:/Dropbox Main/Dropbox/datasets/Trials/match/' + str(match_idx) + '/' + batch[5][idx].split('\\\\')[-1])\n",
    "                shutil.copy(batch[6][idx], 'D:/Dropbox Main/Dropbox/datasets/Trials/match/' + str(match_idx) + '/' + batch[6][idx].split('\\\\')[-1])\n",
    "                fout = open('D:/Dropbox Main/Dropbox/datasets/Trials/match/' + str(match_idx) + '/info.txt', 'w')\n",
    "                fout.write('Gender = %s\\nDB Name = %s\\n' %(batch[3][idx], batch[-1][idx]))\n",
    "                fout.close()\n",
    "                match_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
